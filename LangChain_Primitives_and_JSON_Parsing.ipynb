{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR4QsSCvIdOr"
      },
      "source": [
        "Nihar Lohar\n",
        "Btech AI\n",
        "LAB 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aim:\n",
        " Exploring basic concepts of LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Task 1: Creating API Key, setting up an environment and using LangChain’s model primitive\n",
        "\n",
        "· Create an account and get your API key (Mistral AI, Gemini or any other)\n",
        "\n",
        "· Set up the environment (use either Colab or Jupyter notebook)\n",
        "\n",
        "· Install LangChain (Installation steps will vary based on the environment)\n",
        "\n",
        "· Maka first call to the specific llm model using LangChain’s model primitive (e.g. ChatMistralAI from langchain_mistralai). Write a prompt and get the response.\n",
        "\n",
        "Task 2: Using LangChain PromptTemplates\n",
        "\n",
        "· Craft a prompt and test it.\n",
        "\n",
        "· Build a chain (Prompt → Model)\n",
        "\n",
        "· Add an output parser (text only)\n",
        "\n",
        "· Rerun with temperature=0.1 vs 0.9. How do tone and variety change?\n",
        "\n",
        "Task 3: Structured JSON (JSON output parsing)\n",
        "\n",
        "· from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "· Write a suitable prompt\n",
        "\n",
        "· Use parser = JsonOutputParser()\n",
        "\n",
        "· Create a json_chain\n",
        "\n",
        "· Show the output\n",
        "\n",
        "· Explore how you can make the model output a fixed schema for your course content (e.g., learning_objectives, prerequisites, time_required_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "o8mRVZN67JnF",
        "outputId": "1fb3f18e-181a-497c-95fc-48ff0bd7af9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install -U langchain langchain-core langchain-community langchain-groq groq pydantic\n",
        "\n",
        "import os, json\n",
        "from typing import Dict, Any\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h7dFuJqmGh-D",
        "outputId": "7c4f6992-fbb5-4ece-d4de-660f8f7112c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your API_KEY : ··········\n"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Paste your API_KEY : \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-DJFteDL8pDv"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"qwen/qwen3-32b\",\n",
        "    temperature=0.7\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yC9ggO568pHT",
        "outputId": "2ee50bb4-bb11-411e-a738-e8df2594112f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, the user wants a motivational study tip in one sentence. Let me think about what makes a study tip both motivational and effective. They probably need something that's easy to remember and can boost their morale.\n",
            "\n",
            "First, the tip should encourage consistent effort. Words like \"small steps\" or \"progress\" might work. Maybe emphasize that every bit of work adds up. Also, positivity is key here. Phrases like \"celebrate every small victory\" could help in maintaining motivation. \n",
            "\n",
            "I should avoid clichés that might not feel genuine. Instead, focus on the process rather than the outcome. Maybe mention the importance of showing up each day. \n",
            "\n",
            "Let me check if there's a common study tip that's both motivational and concise. Oh, the idea of progress over perfection. Yeah, that's a good angle. People often feel overwhelmed by big goals, so focusing on small steps can make it manageable. \n",
            "\n",
            "Putting it all together: Something like, \"Celebrate every small step forward, because consistent progress—not perfection—turns your study goals into achievements.\" Does that cover motivation and the study aspect? It highlights consistency, acknowledges effort, and links progress to success. \n",
            "\n",
            "Wait, maybe \"small step\" instead of \"small steps\"? Either way, the message is clear. Let me make sure it's one sentence. Yeah, it is. Alright, that should work.\n",
            "</think>\n",
            "\n",
            "\"Break your study sessions into manageable chunks, and celebrate each small victory—progress builds momentum, and momentum fuels success!\"\n"
          ]
        }
      ],
      "source": [
        "resp = llm.invoke(\"Give me a motivational study tip in one sentence.\")\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OKWU80lL8pN-",
        "outputId": "b75fc698-d424-4ac3-acbd-df493f1c6d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, I need to summarize the basics of LangChain into three bullet points. Let me start by recalling what I know about LangChain.\n",
            "\n",
            "First, I remember that LangChain is a framework for working with large language models. It probably helps in building applications using LLMs. So one point might be about its purpose as a framework for developing LLM-powered apps.\n",
            "\n",
            "Next, the second point. LangChain has components like prompts, models, memory, and indexes. Oh right, and it connects them together. Maybe it's about the key components and how they integrate to handle data and memory, making it easier to manage complex workflows.\n",
            "\n",
            "Third, I think LangChain provides tools for interacting with external data sources and APIs. Like, it can fetch data from a database or use a web search API. So the third bullet could be about the tools for integrating with external systems and data sources to enhance the application's capabilities.\n",
            "\n",
            "Wait, let me check if I missed anything. Oh, maybe the ability to chain together different components. Like, using a prompt, then an LLM, then storing the result in memory or a database. That's part of the framework's design. So maybe the third point is more about the integration with external systems and APIs.\n",
            "\n",
            "I need to make sure each bullet is concise and captures the main idea. Let me rephrase them:\n",
            "\n",
            "1. LangChain is a framework for developing applications powered by large language models, offering tools for building, testing, and deploying such applications.\n",
            "2. It includes core components like prompts, memory, models, and indexes, which work together to manage data, context, and workflows efficiently.\n",
            "3. Provides integration capabilities with external data sources, APIs, and databases, enabling enhanced functionality and real-time data interaction.\n",
            "\n",
            "Hmm, maybe the second point can be more about how the components are structured. Like, it's for creating applications with LLMs by combining different modules. The third point is about the external integrations. I think that's right. Let me make sure each bullet is a separate concept. Yeah, that should cover the basics.\n",
            "</think>\n",
            "\n",
            "- **Framework for LLM-Powered Apps**: LangChain is a framework designed to simplify building applications with large language models (LLMs), offering tools for development, testing, and deployment.  \n",
            "- **Modular Components**: It integrates core modules like prompts, memory, models, and indexes, enabling structured workflows for data processing, context management, and multi-step reasoning.  \n",
            "- **External System Integration**: Supports seamless interaction with external data sources, APIs, databases, and tools (e.g., web search, SQL), enhancing applications with real-time data and external functionality.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI tutor.\"),\n",
        "    (\"human\", \"Summarize {topic} in 3 bullet points.\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "print(chain.invoke({\"topic\":\"LangChain basics\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BJyDfogXDZAt",
        "outputId": "4bcae552-2958-480f-fd97-8f8d1ecbb8d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T=0.1 ➜ <think>\n",
            "Okay, the user wants a summary of staying focused while studying in three bullet points. Let me start by recalling the key strategies for maintaining focus. First, eliminating distractions is crucial. That includes things like turning off notifications, finding a quiet space, and maybe using tools like website blockers. That's a solid first point.\n",
            "\n",
            "Next, breaking tasks into smaller chunks. When studying, large tasks can be overwhelming, so using techniques like the Pomodoro method—working in short intervals with breaks—can help maintain concentration. That makes sense as the second point.\n",
            "\n",
            "Third, staying motivated. This could involve setting clear goals, rewarding oneself after completing tasks, and reminding oneself of the long-term benefits. Maybe also mentioning staying hydrated and taking short breaks to keep energy levels up. That covers the third point.\n",
            "\n",
            "Wait, should I mention physical aspects like environment or posture? The user asked for three bullet points, so maybe keep it concise. The main ideas are minimizing distractions, task management, and motivation. Let me check if these points are comprehensive enough. Yes, they cover the environment, study techniques, and personal motivation. That should work. Let me phrase them clearly and concisely now.\n",
            "</think>\n",
            "\n",
            "- **Minimize distractions** by creating a quiet, organized study space and turning off notifications or using focus tools (e.g., website blockers).  \n",
            "- **Break tasks into smaller goals** using techniques like the Pomodoro method (25-minute focused work + 5-minute breaks) to maintain momentum.  \n",
            "- **Stay motivated** by setting clear objectives, rewarding progress, and reminding yourself of long-term goals to sustain mental energy.\n",
            "T=0.9 ➜ <think>\n",
            "Okay, the user wants a summary of staying focused while studying in three bullet points. Let me start by recalling the key strategies for maintaining focus. First, eliminating distractions is crucial. That includes things like turning off notifications, finding a quiet study space, or using tools like focus apps. I should mention specific examples to make it actionable.\n",
            "\n",
            "Next, time management techniques are important. The Pomodoro technique comes to mind—working in intervals with short breaks. That helps maintain concentration without burnout. Also, setting clear goals for each study session can keep the mind engaged and on track.\n",
            "\n",
            "The third point is about the study environment. A clean, organized space can reduce mental clutter. Using physical tools like highlighters or note-taking can also help. Maybe include something about physical activity or movement breaks to refresh the mind. Wait, the user asked for three bullet points, so each should be concise but cover the main aspects. Let me structure each point with a key strategy and a brief explanation. Need to ensure clarity and practicality without being too vague. Alright, that should cover the essentials for staying focused.\n",
            "</think>\n",
            "\n",
            "- **Eliminate Distractions:** Turn off notifications, choose a quiet environment, and use tools like website blockers to minimize interruptions.  \n",
            "- **Use Time Management Techniques:** Apply methods like the Pomodoro Technique (25-minute focused work + 5-minute breaks) to maintain rhythm and avoid burnout.  \n",
            "- **Stay Organized and Purposeful:** Set clear study goals, break tasks into smaller steps, and take physical notes to reinforce focus and retention.\n"
          ]
        }
      ],
      "source": [
        "llm_low = ChatGroq(model=\"qwen/qwen3-32b\", temperature=0.1)\n",
        "llm_high = ChatGroq(model=\"qwen/qwen3-32b\", temperature=0.9)\n",
        "\n",
        "chain_low  = prompt | llm_low  | StrOutputParser()\n",
        "chain_high = prompt | llm_high | StrOutputParser()\n",
        "\n",
        "q = {\"style\":\"helpful\", \"topic\":\"staying focused while studying\"}\n",
        "\n",
        "print(\"T=0.1 ➜\", chain_low.invoke(q))\n",
        "print(\"T=0.9 ➜\", chain_high.invoke(q))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypNvGac9JS_K"
      },
      "source": [
        "#Inference:\n",
        "\n",
        "At temperature 0.1, the answer stays consistent and straight to the point, giving clear but predictable advice. At temperature 0.9, the answer feels more open and creative, adding extra examples and varied wording, though sometimes less focused."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ys4PtpMoDmTT",
        "outputId": "b9a7939f-70bb-40b7-f40d-df64044e3180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'learning_objectives': ['Understand the core concepts of LangChain and its role in building AI applications', 'Learn how to integrate LangChain with various language models (LLMs)', 'Master the use of LangChain for creating chains, agents, and memory systems', 'Implement LangChain tools for data retrieval, processing, and response generation', 'Troubleshoot common issues and optimize LangChain workflows'], 'prerequisites': ['Basic knowledge of Python programming', 'Familiarity with API concepts and RESTful services', 'Understanding of machine learning fundamentals', 'Experience with prompt engineering or working with LLMs', 'Installation of required libraries (e.g., langchain, transformers)'], 'time_required_min': 180}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.output_parsers import OutputFixingParser\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm_json = ChatGroq(model=\"qwen/qwen3-32b\", temperature=0)\n",
        "base_parser = JsonOutputParser()\n",
        "fixing_parser = OutputFixingParser.from_llm(parser=base_parser, llm=llm_json)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Output only valid JSON with the required keys.\"),\n",
        "    (\"human\",\n",
        "     \"\"\"Topic: {topic}\n",
        "Keys: learning_objectives (string[]), prerequisites (string[]), time_required_min (integer).\"\"\")\n",
        "])\n",
        "\n",
        "robust_chain = prompt | llm_json | fixing_parser\n",
        "print(robust_chain.invoke({\"topic\": \"LangChain\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tAW2Fs7iDZEC",
        "outputId": "eae4a59d-10df-4a71-faa2-49677c43fe8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning_objectives=['Understand the core concepts and components of LangChain', 'Learn how to install and set up LangChain in a Python environment', 'Explore the integration of LangChain with various language models (LLMs)', 'Develop skills to create and manage prompts for effective model interactions', 'Implement memory and state management in LangChain applications', \"Build simple chains and agents using LangChain's modular components\", 'Apply LangChain tools for database and API integrations'] prerequisites=['Basic knowledge of Python programming', 'Familiarity with machine learning and large language models (LLMs)', 'Understanding of API concepts and RESTful services', 'Experience with Jupyter Notebooks or Python IDEs', 'Optional: Prior exposure to frameworks like Hugging Face or TensorFlow'] time_required_min=120\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "class CourseSchema(BaseModel):\n",
        "    learning_objectives: list[str] = Field(description=\"Learning objectives of the course\")\n",
        "    prerequisites: list[str] = Field(description=\"What the student should already know\")\n",
        "    time_required_min: int = Field(description=\"Estimated time in minutes\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=CourseSchema)\n",
        "\n",
        "schema_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a course generator.\"),\n",
        "    (\"human\", \"Generate course info for {topic}.\\n{format_instructions}\")\n",
        "]).partial(format_instructions=parser.get_format_instructions())\n",
        "\n",
        "schema_chain = schema_prompt | llm | parser\n",
        "\n",
        "print(schema_chain.invoke({\"topic\":\"Introduction to LangChain\"}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPDQJ-QMJiYI"
      },
      "source": [
        "#What does LangChain give you beyond calling the HTTP API directly?\n",
        "\n",
        "LangChain provides a higher-level framework so you don’t have to manage raw API calls yourself. It makes it easier to:\n",
        "\n",
        "Swap between different model providers with minimal code changes.\n",
        "\n",
        "Build structured workflows using PromptTemplates, chains, and parsers instead\n",
        "of string-concatenated prompts.\n",
        "\n",
        "Maintain conversation history (memory) without manually tracking it.\n",
        "\n",
        "Enforce reliable outputs as text, JSON, or typed schemas.\n",
        "\n",
        "Add post-processing, retrieval, or tool use in a better way.\n",
        "\n",
        "\n",
        "#When would you prefer plain text vs structured (JSON) outputs?\n",
        "I would prefer plain text when the response is meant to be read directly by people, for example in explanations, summaries, or general study help.\n",
        "\n",
        "I would prefer structured JSON output when the response needs to be used by another system or application, such as storing course details, creating checklists, or feeding data into a dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EjyMc2UKJkt0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
