{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua0vqhuTB-HE"
      },
      "source": [
        "#Nihar Lohar\n",
        "#BTech AI\n",
        "Generative AI\n",
        "\n",
        "Lab 9\n",
        "\n",
        "\n",
        "# Aim: \n",
        "To implement a Retrieval-Augmented Generation (RAG) pipeline using a vector database, with emphasis on chunking techniques and HNSW algorithm for nearest neighbor search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Task 1: Dataset Preparation\n",
        "\n",
        "1. Select a text dataset (e.g., 2–3 Wikipedia articles, course notes, or research papers).\n",
        "\n",
        "2. Apply different chunking strategies:\n",
        "\n",
        "a. Fixed-size Chunking\n",
        "\n",
        "i. Split text into chunks of N tokens/characters.\n",
        "\n",
        "ii. Simple but may break sentences mid-way.\n",
        "\n",
        "iii. Example: 200–300 tokens per chunk.\n",
        "\n",
        "b. Sentence-based Chunking\n",
        "\n",
        "i. Split text into individual sentences or groups of 2–3 sentences.\n",
        "\n",
        "ii. Maintains semantic meaning per chunk.\n",
        "\n",
        "iii. Good for FAQs or structured text.\n",
        "\n",
        "c. Paragraph-based Chunking (Semantic Chunking)\n",
        "\n",
        "i. Split based on paragraph boundaries.\n",
        "\n",
        "ii. Preserves natural structure and coherence.\n",
        "\n",
        "iii. Best for articles, blogs, or research papers.\n",
        "\n",
        "d. Sliding Window (Overlapping Chunks)\n",
        "\n",
        "i. Use overlaps between chunks (e.g., 200 tokens with 50-token overlap).\n",
        "\n",
        "ii. Ensures that information spanning chunk boundaries isn’t lost.\n",
        "\n",
        "iii. Useful in tasks like Q&A where context may cross chunk boundaries.\n",
        "\n",
        "e. Heading-based Chunking\n",
        "\n",
        "i. Split text by document headings / subheadings.\n",
        "\n",
        "\n",
        "ii. Works well with structured data (textbooks, research papers, Wikipedia).\n",
        "\n",
        "iii. Ensures each chunk represents a logical section.\n",
        "\n",
        "f. Semantic/Embedding-based Chunking\n",
        "\n",
        "i. Use sentence embeddings + similarity to merge closely related sentences into one chunk.\n",
        "\n",
        "ii. Requires clustering or similarity threshold.\n",
        "\n",
        "iii. Produces contextually meaningful chunks.\n",
        "\n",
        "g. Hybrid Chunking\n",
        "\n",
        "i. Combine methods (e.g., fixed-size with overlap + heading detection).\n",
        "\n",
        "ii. Gives balance between coverage and coherence.\n",
        "\n",
        "3. Store the resulting chunks and note their average length.\n",
        "\n",
        "4. Compare with sample queries:\n",
        "\n",
        "i. Which strategy retrieves more relevant chunks?\n",
        "\n",
        "ii. Which one leads to fewer “broken thoughts”?\n",
        "\n",
        "Task 2: Embedding & Vector Storage\n",
        "\n",
        "1. Use a pre-trained embedding model (all-MiniLM-L6-v2 or similar).\n",
        "\n",
        "2. Store embeddings in a vector database (Chroma / FAISS / Weaviate).\n",
        "\n",
        "3. Ensure the DB uses HNSW index.\n",
        "\n",
        "4. Insert chunks with metadata (chunk id, source text).\n",
        "\n",
        "Task 3: Retrieval with HNSW\n",
        "\n",
        "1. Implement top-k retrieval using HNSW search.\n",
        "\n",
        "2. Compare results for different efSearch (query-time search breadth) values (e.g., 20, 100, 200).\n",
        "\n",
        "3. Build indexes with efConstruction (indexing quality) = 50, 200, 500 and compare retrieval results.\n",
        "\n",
        "4. Try M (max connections per node) = 8, 16, 32 and compare accuracy vs memory usage.\n",
        "\n",
        "5. Query example: “What is the role of HNSW in vector search?”\n",
        "\n",
        "6. Observe how different chunking strategies affect retrieved results.\n",
        "\n",
        "Task 4: RAG Pipeline\n",
        "\n",
        "1. Implement a simple RAG loop:\n",
        "\n",
        "a. Take a user query.\n",
        "\n",
        "b. Retrieve top-k chunks using HNSW.\n",
        "\n",
        "c. Pass retrieved context + query into an LLM (Gemini, Mistral or any other free API model).\n",
        "\n",
        "d. Display generated answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmHA8yhv5wo-"
      },
      "outputs": [],
      "source": [
        "pip install numpy\n",
        "pip install nltk\n",
        "pip install sentence-transformers\n",
        "pip install faiss-cpu\n",
        "pip install requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2y5xYgX50G9"
      },
      "outputs": [],
      "source": [
        "pip install faiss-gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-029H3t5yJi"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cGPl0tux6G9x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WBMOmtq46Imy"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "inEssyHp6KMJ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Chunk:\n",
        "    id: str\n",
        "    text: str\n",
        "    source: str\n",
        "    chunk_type: str\n",
        "    start_char: int\n",
        "    end_char: int\n",
        "    token_count: int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BjhgGO454YMT"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Chunking Strategies\n",
        "# -----------------------------\n",
        "class TextChunker:\n",
        "    def __init__(self):\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        return len(text.split())\n",
        "\n",
        "    def fixed_size_chunking(self, text: str, chunk_size: int = 250, source: str = \"\") -> List[Chunk]:\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        for i in range(0, len(words), chunk_size):\n",
        "            chunk_words = words[i:i + chunk_size]\n",
        "            chunks.append(Chunk(\n",
        "                id=f\"fixed_{source}_{i//chunk_size}\",\n",
        "                text=\" \".join(chunk_words),\n",
        "                source=source,\n",
        "                chunk_type=\"fixed_size\",\n",
        "                start_char=len(\" \".join(words[:i])),\n",
        "                end_char=len(\" \".join(words[:i + len(chunk_words)])),\n",
        "                token_count=len(chunk_words)\n",
        "            ))\n",
        "        return chunks\n",
        "\n",
        "    def sentence_based_chunking(self, text: str, sentences_per_chunk: int = 3, source: str = \"\") -> List[Chunk]:\n",
        "        sentences = sent_tokenize(text)\n",
        "        chunks = []\n",
        "        for i in range(0, len(sentences), sentences_per_chunk):\n",
        "            chunk_sentences = sentences[i:i+sentences_per_chunk]\n",
        "            chunks.append(Chunk(\n",
        "                id=f\"sentence_{source}_{i//sentences_per_chunk}\",\n",
        "                text=\" \".join(chunk_sentences),\n",
        "                source=source,\n",
        "                chunk_type=\"sentence_based\",\n",
        "                start_char=0,\n",
        "                end_char=len(\" \".join(chunk_sentences)),\n",
        "                token_count=self.count_tokens(\" \".join(chunk_sentences))\n",
        "            ))\n",
        "        return chunks\n",
        "\n",
        "    def paragraph_based_chunking(self, text: str, source: str = \"\") -> List[Chunk]:\n",
        "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "        chunks = []\n",
        "        for i, paragraph in enumerate(paragraphs):\n",
        "            if len(paragraph) > 50:\n",
        "                chunks.append(Chunk(\n",
        "                    id=f\"paragraph_{source}_{i}\",\n",
        "                    text=paragraph,\n",
        "                    source=source,\n",
        "                    chunk_type=\"paragraph_based\",\n",
        "                    start_char=0,\n",
        "                    end_char=len(paragraph),\n",
        "                    token_count=self.count_tokens(paragraph)\n",
        "                ))\n",
        "        return chunks\n",
        "\n",
        "    def sliding_window_chunking(self, text: str, chunk_size: int = 200, overlap: int = 50, source: str = \"\") -> List[Chunk]:\n",
        "        words = text.split()\n",
        "        step = chunk_size - overlap\n",
        "        chunks = []\n",
        "        for i in range(0, len(words), step):\n",
        "            chunk_words = words[i:i+chunk_size]\n",
        "            if len(chunk_words) < 50:\n",
        "                break\n",
        "            chunks.append(Chunk(\n",
        "                id=f\"sliding_{source}_{i//step}\",\n",
        "                text=\" \".join(chunk_words),\n",
        "                source=source,\n",
        "                chunk_type=\"sliding_window\",\n",
        "                start_char=0,\n",
        "                end_char=len(\" \".join(chunk_words)),\n",
        "                token_count=len(chunk_words)\n",
        "            ))\n",
        "        return chunks\n",
        "\n",
        "    def heading_based_chunking(self, text: str, source: str = \"\") -> List[Chunk]:\n",
        "        sections = re.split(r'\\n(?=#+\\s)', text)\n",
        "        chunks = []\n",
        "        for i, section in enumerate(sections):\n",
        "            section = section.strip()\n",
        "            if len(section) > 100:\n",
        "                chunks.append(Chunk(\n",
        "                    id=f\"heading_{source}_{i}\",\n",
        "                    text=section,\n",
        "                    source=source,\n",
        "                    chunk_type=\"heading_based\",\n",
        "                    start_char=0,\n",
        "                    end_char=len(section),\n",
        "                    token_count=self.count_tokens(section)\n",
        "                ))\n",
        "        return chunks\n",
        "\n",
        "    def semantic_chunking(self, text: str, similarity_threshold: float = 0.7, source: str = \"\") -> List[Chunk]:\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) < 2:\n",
        "            return [Chunk(\n",
        "                id=f\"semantic_{source}_0\",\n",
        "                text=text,\n",
        "                source=source,\n",
        "                chunk_type=\"semantic\",\n",
        "                start_char=0,\n",
        "                end_char=len(text),\n",
        "                token_count=self.count_tokens(text)\n",
        "            )]\n",
        "        embeddings = self.embedding_model.encode(sentences)\n",
        "        chunks = []\n",
        "        current_chunk_sentences = [sentences[0]]\n",
        "        current_embedding = embeddings[0:1]\n",
        "        for i in range(1, len(sentences)):\n",
        "            centroid = np.mean(current_embedding, axis=0)\n",
        "            similarity = np.dot(centroid, embeddings[i]) / (np.linalg.norm(centroid)*np.linalg.norm(embeddings[i]))\n",
        "            if similarity > similarity_threshold:\n",
        "                current_chunk_sentences.append(sentences[i])\n",
        "                current_embedding = np.vstack([current_embedding, embeddings[i:i+1]])\n",
        "            else:\n",
        "                chunks.append(Chunk(\n",
        "                    id=f\"semantic_{source}_{len(chunks)}\",\n",
        "                    text=\" \".join(current_chunk_sentences),\n",
        "                    source=source,\n",
        "                    chunk_type=\"semantic\",\n",
        "                    start_char=0,\n",
        "                    end_char=len(\" \".join(current_chunk_sentences)),\n",
        "                    token_count=self.count_tokens(\" \".join(current_chunk_sentences))\n",
        "                ))\n",
        "                current_chunk_sentences = [sentences[i]]\n",
        "                current_embedding = embeddings[i:i+1]\n",
        "        if current_chunk_sentences:\n",
        "            chunks.append(Chunk(\n",
        "                id=f\"semantic_{source}_{len(chunks)}\",\n",
        "                text=\" \".join(current_chunk_sentences),\n",
        "                source=source,\n",
        "                chunk_type=\"semantic\",\n",
        "                start_char=0,\n",
        "                end_char=len(\" \".join(current_chunk_sentences)),\n",
        "                token_count=self.count_tokens(\" \".join(current_chunk_sentences))\n",
        "            ))\n",
        "        return chunks\n",
        "\n",
        "    def hybrid_chunking(self, text: str, source: str = \"\") -> List[Chunk]:\n",
        "        heading_chunks = self.heading_based_chunking(text, source)\n",
        "        if len(heading_chunks) > 1:\n",
        "            return heading_chunks\n",
        "        else:\n",
        "            return self.sliding_window_chunking(text, source=source)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "WRkEw_sG6YLU"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Vector Store + HNSW\n",
        "# -----------------------------\n",
        "class VectorStore:\n",
        "    def __init__(self, embedding_dim: int = 384):\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.chunks: List[Chunk] = []\n",
        "        self.index = None\n",
        "\n",
        "    def create_hnsw_index(self, M: int = 16, ef_construction: int = 200):\n",
        "        self.index = faiss.IndexHNSWFlat(self.embedding_dim, M)\n",
        "        self.index.hnsw.efConstruction = ef_construction\n",
        "\n",
        "    def add_chunks(self, chunks: List[Chunk]):\n",
        "        if not chunks:\n",
        "            return\n",
        "        self.chunks.extend(chunks)\n",
        "        texts = [chunk.text for chunk in chunks]\n",
        "        embeddings = self.embedding_model.encode(texts).astype('float32')\n",
        "        if self.index is None:\n",
        "            self.create_hnsw_index()\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "    def search(self, query: str, k: int = 5, ef_search: int = 50) -> List[Tuple[Chunk, float]]:\n",
        "        if not self.index or len(self.chunks) == 0:\n",
        "            return []\n",
        "        self.index.hnsw.efSearch = ef_search\n",
        "        query_emb = self.embedding_model.encode([query]).astype('float32')\n",
        "        scores, indices = self.index.search(query_emb, k)\n",
        "        results = [(self.chunks[i], float(score)) for i, score in zip(indices[0], scores[0]) if i < len(self.chunks)]\n",
        "        return results\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        token_counts = [c.token_count for c in self.chunks]\n",
        "        chunk_types = defaultdict(int)\n",
        "        for c in self.chunks:\n",
        "            chunk_types[c.chunk_type] += 1\n",
        "        return {\n",
        "            \"total_chunks\": len(self.chunks),\n",
        "            \"chunk_types\": dict(chunk_types),\n",
        "            \"avg_tokens\": np.mean(token_counts) if token_counts else 0,\n",
        "            \"min_tokens\": np.min(token_counts) if token_counts else 0,\n",
        "            \"max_tokens\": np.max(token_counts) if token_counts else 0\n",
        "        }\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Pm3OtJRP61xX"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# LLM Client\n",
        "# -----------------------------\n",
        "class LLMClient:\n",
        "    def __init__(self, api_key: str = None):\n",
        "        self.model_name = 'qwen/qwen3-32b'\n",
        "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "    def set_api_key_interactive(self):\n",
        "        import getpass\n",
        "        try:\n",
        "            self.api_key = getpass.getpass(\"Enter your GROQ API Key: \")\n",
        "        except Exception:\n",
        "            self.api_key = input(\"Enter your GROQ API Key: \")\n",
        "\n",
        "    def call_llm(self, prompt: str) -> str:\n",
        "        import requests\n",
        "        if not self.api_key:\n",
        "            return \"Error: API key not set. Please call set_api_key_interactive() first.\"\n",
        "        headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\"}\n",
        "        data = {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant you have to refer the document and answer.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            \"max_tokens\": 1000,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "                headers=headers,\n",
        "                json=data,\n",
        "                timeout=30\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as e:\n",
        "            return f\"LLM API Error: {e}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSY9E1Af76R0",
        "outputId": "5df162df-b046-4eaa-ec02-a395e7ce8165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "llm = LLMClient()\n",
        "llm.set_api_key_interactive()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "aNeNAB1C6f2n"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# RAG Pipeline\n",
        "# -----------------------------\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        self.chunker = TextChunker()\n",
        "        self.vector_stores: Dict[str, VectorStore] = {}\n",
        "        self.llm = LLMClient()\n",
        "\n",
        "    def process_documents(self, docs: Dict[str, str]):\n",
        "        methods = {\n",
        "            \"fixed_size\": self.chunker.fixed_size_chunking,\n",
        "            \"sentence_based\": self.chunker.sentence_based_chunking,\n",
        "            \"paragraph_based\": self.chunker.paragraph_based_chunking,\n",
        "            \"sliding_window\": self.chunker.sliding_window_chunking,\n",
        "            \"heading_based\": self.chunker.heading_based_chunking,\n",
        "            \"semantic\": self.chunker.semantic_chunking,\n",
        "            \"hybrid\": self.chunker.hybrid_chunking\n",
        "        }\n",
        "        for name, func in methods.items():\n",
        "            print(f\"\\nProcessing: {name}\")\n",
        "            store = VectorStore()\n",
        "            all_chunks = []\n",
        "            for source, text in docs.items():\n",
        "                chunks = func(text, source=source)\n",
        "                all_chunks.extend(chunks)\n",
        "            store.add_chunks(all_chunks)\n",
        "            stats = store.get_stats()\n",
        "            print(f\"  {stats['total_chunks']} chunks, avg tokens: {stats['avg_tokens']:.1f}\")\n",
        "            for chunk in store.chunks:\n",
        "                preview = chunk.text[:100] + \"...\" if len(chunk.text) > 100 else chunk.text\n",
        "                print(f\"    {chunk.id} | Tokens: {chunk.token_count} | Preview: {preview}\")\n",
        "            self.vector_stores[name] = store\n",
        "\n",
        "    def rag_loop(self, method: str = 'semantic', k: int = 3):\n",
        "        print(f\"\\nStarting RAG loop with '{method}' chunking\")\n",
        "        while True:\n",
        "            query = input(\" Query (type 'quit' to exit): \").strip()\n",
        "            if query.lower() == 'quit':\n",
        "                break\n",
        "            if method not in self.vector_stores:\n",
        "                print(f\"Method {method} not available\")\n",
        "                continue\n",
        "            store = self.vector_stores[method]\n",
        "            retrieved = store.search(query, k=k)\n",
        "            if not retrieved:\n",
        "                print(\"No relevant chunks found\")\n",
        "                continue\n",
        "            context = \"\\n\\n\".join([f\"{i+1}. {chunk.text}\" for i, (chunk, _) in enumerate(retrieved)])\n",
        "            prompt = f\"CONTEXT:\\n{context}\\n\\nQUESTION: {query}\\nANSWER:\"\n",
        "            answer = self.llm.call_llm(prompt)\n",
        "            print(\"\\n Answer:\")\n",
        "            print(answer)\n",
        "            print(\"\\n Retrieved Chunks:\")\n",
        "            for i, (chunk, score) in enumerate(retrieved):\n",
        "                preview = chunk.text[:150] + \"...\" if len(chunk.text) > 150 else chunk.text\n",
        "                print(f\"{i+1}. Score: {score:.3f}, Tokens: {chunk.token_count}, Preview: {preview}\")\n",
        "            print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gulv6cTe9GYn",
        "outputId": "7f3f4fb1-64e3-4e78-e21d-d67db2bef80b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "llm = LLMClient()\n",
        "llm.set_api_key_interactive()\n",
        "\n",
        "rag = RAGPipeline()\n",
        "rag.llm = llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS-igv0R6mWm",
        "outputId": "88a2db67-7c8c-4604-873f-53e27abc284e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API Key: ··········\n",
            "\n",
            "Processing: fixed_size\n",
            "  4 chunks, avg tokens: 232.5\n",
            "    fixed_ai_doc_0 | Tokens: 250 | Preview: # Artificial Intelligence Artificial Intelligence (AI) is a multidisciplinary field of computer scie...\n",
            "    fixed_ai_doc_1 | Tokens: 250 | Preview: representations from large-scale data. It powers modern AI applications like speech recognition, ima...\n",
            "    fixed_ai_doc_2 | Tokens: 250 | Preview: domestic help, hospitality, and customer service. - **Autonomous Vehicles**: Self-driving cars, dron...\n",
            "    fixed_ai_doc_3 | Tokens: 180 | Preview: trust their decisions. - **Privacy**: Protecting sensitive personal data and preventing misuse. - **...\n",
            "\n",
            "Processing: sentence_based\n",
            "  19 chunks, avg tokens: 48.9\n",
            "    sentence_ai_doc_0 | Tokens: 68 | Preview: \n",
            "\n",
            "        # Artificial Intelligence\n",
            "\n",
            "Artificial Intelligence (AI) is a multidisciplinary field of co...\n",
            "    sentence_ai_doc_1 | Tokens: 74 | Preview: AI is applied across a wide range of domains including healthcare, finance, education, transportatio...\n",
            "    sentence_ai_doc_2 | Tokens: 41 | Preview: ML techniques are broadly categorized into:\n",
            "\n",
            "- **Supervised Learning**: Algorithms learn from labele...\n",
            "    sentence_ai_doc_3 | Tokens: 46 | Preview: Examples include clustering customers based on purchasing behavior or anomaly detection in network s...\n",
            "    sentence_ai_doc_4 | Tokens: 54 | Preview: - **Deep Learning**: A subfield of ML based on neural networks with many layers (deep neural network...\n",
            "    sentence_ai_doc_5 | Tokens: 50 | Preview: Key challenges include overfitting, underfitting, interpretability, and scalability. ## Natural Lang...\n",
            "    sentence_ai_doc_6 | Tokens: 53 | Preview: Common applications include:\n",
            "\n",
            "- **Chatbots and Virtual Assistants**: AI-powered conversational agent...\n",
            "    sentence_ai_doc_7 | Tokens: 55 | Preview: - **Text Summarization**: Automatically creating concise summaries of large documents or articles. -...\n",
            "    sentence_ai_doc_8 | Tokens: 52 | Preview: ## Robotics\n",
            "\n",
            "Robotics is an interdisciplinary field that integrates AI with mechanical, electrical, ...\n",
            "    sentence_ai_doc_9 | Tokens: 40 | Preview: - **Service Robotics**: Robots designed for healthcare, domestic help, hospitality, and customer ser...\n",
            "    sentence_ai_doc_10 | Tokens: 54 | Preview: - **Medical Robotics**: Surgical robots, rehabilitation systems, and assistive devices for patients....\n",
            "    sentence_ai_doc_11 | Tokens: 45 | Preview: It combines image processing, pattern recognition, and machine learning techniques to extract meanin...\n",
            "    sentence_ai_doc_12 | Tokens: 39 | Preview: - **Facial Recognition**: Identifying or verifying individuals based on facial features. - **Medical...\n",
            "    sentence_ai_doc_13 | Tokens: 52 | Preview: - **Augmented Reality and Computer Graphics**: Enhancing real-world environments with computer-gener...\n",
            "    sentence_ai_doc_14 | Tokens: 39 | Preview: Ethical considerations include:\n",
            "\n",
            "- **Fairness**: Avoiding discrimination and ensuring equitable trea...\n",
            "    sentence_ai_doc_15 | Tokens: 37 | Preview: - **Bias Mitigation**: Identifying and reducing biases in training data and AI models. - **Accountab...\n",
            "    sentence_ai_doc_16 | Tokens: 63 | Preview: Ethical AI requires collaboration between technologists, policymakers, ethicists, and society to ens...\n",
            "    sentence_ai_doc_17 | Tokens: 40 | Preview: - **AI in Healthcare**: Improving diagnostics, personalized medicine, and drug discovery. - **AI for...\n",
            "    sentence_ai_doc_18 | Tokens: 28 | Preview: The future of AI promises to transform industries, enhance human potential, and address global chall...\n",
            "\n",
            "Processing: paragraph_based\n",
            "  20 chunks, avg tokens: 45.4\n",
            "    paragraph_ai_doc_1 | Tokens: 65 | Preview: Artificial Intelligence (AI) is a multidisciplinary field of computer science and engineering focuse...\n",
            "    paragraph_ai_doc_2 | Tokens: 40 | Preview: AI is applied across a wide range of domains including healthcare, finance, education, transportatio...\n",
            "    paragraph_ai_doc_4 | Tokens: 37 | Preview: Machine learning (ML) is a core subset of AI that focuses on creating algorithms that allow computer...\n",
            "    paragraph_ai_doc_5 | Tokens: 120 | Preview: - **Supervised Learning**: Algorithms learn from labeled datasets to make predictions or classify da...\n",
            "    paragraph_ai_doc_6 | Tokens: 23 | Preview: Machine learning models are trained using optimization techniques to minimize errors and maximize pr...\n",
            "    paragraph_ai_doc_8 | Tokens: 41 | Preview: Natural Language Processing (NLP) is a branch of AI that enables computers to understand, interpret,...\n",
            "    paragraph_ai_doc_9 | Tokens: 81 | Preview: - **Chatbots and Virtual Assistants**: AI-powered conversational agents like virtual assistants, cus...\n",
            "    paragraph_ai_doc_10 | Tokens: 24 | Preview: NLP also involves challenges such as understanding context, idioms, sarcasm, and ambiguity in human ...\n",
            "    paragraph_ai_doc_12 | Tokens: 37 | Preview: Robotics is an interdisciplinary field that integrates AI with mechanical, electrical, and software ...\n",
            "    paragraph_ai_doc_13 | Tokens: 65 | Preview: - **Industrial Robotics**: Robots used in manufacturing, assembly lines, packaging, and precision ta...\n",
            "    paragraph_ai_doc_14 | Tokens: 17 | Preview: Challenges in robotics include real-time perception, mobility in unstructured environments, manipula...\n",
            "    paragraph_ai_doc_16 | Tokens: 45 | Preview: Computer vision is a branch of AI focused on enabling machines to interpret, understand, and respond...\n",
            "    paragraph_ai_doc_17 | Tokens: 73 | Preview: - **Object Detection**: Identifying and localizing objects within images or videos.\n",
            "- **Image Classi...\n",
            "    paragraph_ai_doc_18 | Tokens: 20 | Preview: Computer vision systems face challenges such as variations in lighting, occlusion, background clutte...\n",
            "    paragraph_ai_doc_20 | Tokens: 19 | Preview: Ethics in AI is a critical aspect that ensures AI technologies are developed and deployed responsibl...\n",
            "    paragraph_ai_doc_21 | Tokens: 73 | Preview: - **Fairness**: Avoiding discrimination and ensuring equitable treatment across different groups of ...\n",
            "    paragraph_ai_doc_22 | Tokens: 20 | Preview: Ethical AI requires collaboration between technologists, policymakers, ethicists, and society to ens...\n",
            "    paragraph_ai_doc_24 | Tokens: 12 | Preview: AI continues to evolve rapidly, with research focused on areas such as:\n",
            "    paragraph_ai_doc_25 | Tokens: 68 | Preview: - **General AI**: Creating machines with human-level cognitive abilities across a broad range of tas...\n",
            "    paragraph_ai_doc_26 | Tokens: 28 | Preview: The future of AI promises to transform industries, enhance human potential, and address global chall...\n",
            "\n",
            "Processing: sliding_window\n",
            "  6 chunks, avg tokens: 196.7\n",
            "    sliding_ai_doc_0 | Tokens: 200 | Preview: # Artificial Intelligence Artificial Intelligence (AI) is a multidisciplinary field of computer scie...\n",
            "    sliding_ai_doc_1 | Tokens: 200 | Preview: Learning**: Algorithms learn from labeled datasets to make predictions or classify data. Common appl...\n",
            "    sliding_ai_doc_2 | Tokens: 200 | Preview: a branch of AI that enables computers to understand, interpret, and generate human language. NLP com...\n",
            "    sliding_ai_doc_3 | Tokens: 200 | Preview: AI with mechanical, electrical, and software engineering to create autonomous and semi-autonomous ma...\n",
            "    sliding_ai_doc_4 | Tokens: 200 | Preview: information from images and videos. Common tasks and applications include: - **Object Detection**: I...\n",
            "    sliding_ai_doc_5 | Tokens: 180 | Preview: trust their decisions. - **Privacy**: Protecting sensitive personal data and preventing misuse. - **...\n",
            "\n",
            "Processing: heading_based\n",
            "  7 chunks, avg tokens: 132.9\n",
            "    heading_ai_doc_0 | Tokens: 108 | Preview: # Artificial Intelligence\n",
            "\n",
            "Artificial Intelligence (AI) is a multidisciplinary field of computer sci...\n",
            "    heading_ai_doc_1 | Tokens: 183 | Preview: ## Machine Learning\n",
            "\n",
            "Machine learning (ML) is a core subset of AI that focuses on creating algorithm...\n",
            "    heading_ai_doc_2 | Tokens: 150 | Preview: ## Natural Language Processing\n",
            "\n",
            "Natural Language Processing (NLP) is a branch of AI that enables com...\n",
            "    heading_ai_doc_3 | Tokens: 121 | Preview: ## Robotics\n",
            "\n",
            "Robotics is an interdisciplinary field that integrates AI with mechanical, electrical, ...\n",
            "    heading_ai_doc_4 | Tokens: 141 | Preview: ## Computer Vision\n",
            "\n",
            "Computer vision is a branch of AI focused on enabling machines to interpret, und...\n",
            "    heading_ai_doc_5 | Tokens: 116 | Preview: ## Ethics and Bias\n",
            "\n",
            "Ethics in AI is a critical aspect that ensures AI technologies are developed and...\n",
            "    heading_ai_doc_6 | Tokens: 111 | Preview: ## Future Directions\n",
            "\n",
            "AI continues to evolve rapidly, with research focused on areas such as:\n",
            "\n",
            "- **G...\n",
            "\n",
            "Processing: semantic\n",
            "  54 chunks, avg tokens: 17.2\n",
            "    semantic_ai_doc_0 | Tokens: 30 | Preview: \n",
            "\n",
            "        # Artificial Intelligence\n",
            "\n",
            "Artificial Intelligence (AI) is a multidisciplinary field of co...\n",
            "    semantic_ai_doc_1 | Tokens: 15 | Preview: These tasks include reasoning, learning, problem-solving, perception, language understanding, decisi...\n",
            "    semantic_ai_doc_2 | Tokens: 23 | Preview: AI systems leverage large amounts of data, advanced algorithms, and computational power to mimic cog...\n",
            "    semantic_ai_doc_3 | Tokens: 18 | Preview: AI is applied across a wide range of domains including healthcare, finance, education, transportatio...\n",
            "    semantic_ai_doc_4 | Tokens: 22 | Preview: The ultimate goal of AI is to build machines that can perform complex tasks autonomously and intelli...\n",
            "    semantic_ai_doc_5 | Tokens: 54 | Preview: ## Machine Learning\n",
            "\n",
            "Machine learning (ML) is a core subset of AI that focuses on creating algorithm...\n",
            "    semantic_ai_doc_6 | Tokens: 10 | Preview: Common applications include spam detection, fraud detection, and medical diagnosis.\n",
            "    semantic_ai_doc_7 | Tokens: 11 | Preview: - **Unsupervised Learning**: Algorithms identify patterns or structures in unlabeled data.\n",
            "    semantic_ai_doc_8 | Tokens: 14 | Preview: Examples include clustering customers based on purchasing behavior or anomaly detection in network s...\n",
            "    semantic_ai_doc_9 | Tokens: 20 | Preview: - **Reinforcement Learning**: Agents learn by interacting with an environment and receiving feedback...\n",
            "    semantic_ai_doc_10 | Tokens: 12 | Preview: This approach is widely used in robotics, game AI, and autonomous driving.\n",
            "    semantic_ai_doc_11 | Tokens: 25 | Preview: - **Deep Learning**: A subfield of ML based on neural networks with many layers (deep neural network...\n",
            "    semantic_ai_doc_12 | Tokens: 14 | Preview: It powers modern AI applications like speech recognition, image classification, and natural language...\n",
            "    semantic_ai_doc_13 | Tokens: 15 | Preview: Machine learning models are trained using optimization techniques to minimize errors and maximize pr...\n",
            "    semantic_ai_doc_14 | Tokens: 8 | Preview: Key challenges include overfitting, underfitting, interpretability, and scalability.\n",
            "    semantic_ai_doc_15 | Tokens: 42 | Preview: ## Natural Language Processing\n",
            "\n",
            "Natural Language Processing (NLP) is a branch of AI that enables com...\n",
            "    semantic_ai_doc_16 | Tokens: 20 | Preview: Common applications include:\n",
            "\n",
            "- **Chatbots and Virtual Assistants**: AI-powered conversational agent...\n",
            "    semantic_ai_doc_17 | Tokens: 17 | Preview: - **Sentiment Analysis**: Determining opinions or emotions in text for marketing, social media monit...\n",
            "    semantic_ai_doc_18 | Tokens: 16 | Preview: - **Language Translation**: Translating text or speech between languages using AI models like neural...\n",
            "    semantic_ai_doc_19 | Tokens: 12 | Preview: - **Text Summarization**: Automatically creating concise summaries of large documents or articles.\n",
            "    semantic_ai_doc_20 | Tokens: 19 | Preview: - **Speech Recognition and Generation**: Converting spoken language to text and vice versa for acces...\n",
            "    semantic_ai_doc_21 | Tokens: 24 | Preview: NLP also involves challenges such as understanding context, idioms, sarcasm, and ambiguity in human ...\n",
            "    semantic_ai_doc_22 | Tokens: 22 | Preview: ## Robotics\n",
            "\n",
            "Robotics is an interdisciplinary field that integrates AI with mechanical, electrical, ...\n",
            "    semantic_ai_doc_23 | Tokens: 14 | Preview: Robotics systems can sense their environment, process information, make decisions, and perform physi...\n",
            "    semantic_ai_doc_24 | Tokens: 16 | Preview: Key areas include:\n",
            "\n",
            "- **Industrial Robotics**: Robots used in manufacturing, assembly lines, packagi...\n",
            "    semantic_ai_doc_25 | Tokens: 13 | Preview: - **Service Robotics**: Robots designed for healthcare, domestic help, hospitality, and customer ser...\n",
            "    semantic_ai_doc_26 | Tokens: 14 | Preview: - **Autonomous Vehicles**: Self-driving cars, drones, and delivery robots that navigate without huma...\n",
            "    semantic_ai_doc_27 | Tokens: 13 | Preview: - **Humanoid Robots**: Robots designed to resemble humans and interact in human-like ways.\n",
            "    semantic_ai_doc_28 | Tokens: 12 | Preview: - **Medical Robotics**: Surgical robots, rehabilitation systems, and assistive devices for patients.\n",
            "    semantic_ai_doc_29 | Tokens: 17 | Preview: Challenges in robotics include real-time perception, mobility in unstructured environments, manipula...\n",
            "    semantic_ai_doc_30 | Tokens: 25 | Preview: ## Computer Vision\n",
            "\n",
            "Computer vision is a branch of AI focused on enabling machines to interpret, und...\n",
            "    semantic_ai_doc_31 | Tokens: 18 | Preview: It combines image processing, pattern recognition, and machine learning techniques to extract meanin...\n",
            "    semantic_ai_doc_32 | Tokens: 16 | Preview: Common tasks and applications include:\n",
            "\n",
            "- **Object Detection**: Identifying and localizing objects w...\n",
            "    semantic_ai_doc_33 | Tokens: 11 | Preview: - **Image Classification**: Categorizing images into predefined classes for recognition purposes.\n",
            "    semantic_ai_doc_34 | Tokens: 11 | Preview: - **Facial Recognition**: Identifying or verifying individuals based on facial features.\n",
            "    semantic_ai_doc_35 | Tokens: 14 | Preview: - **Medical Imaging Analysis**: Detecting diseases and anomalies in X-rays, MRIs, and CT scans.\n",
            "    semantic_ai_doc_36 | Tokens: 14 | Preview: - **Autonomous Navigation**: Allowing vehicles and robots to perceive their surroundings for safe na...\n",
            "    semantic_ai_doc_37 | Tokens: 12 | Preview: - **Augmented Reality and Computer Graphics**: Enhancing real-world environments with computer-gener...\n",
            "    semantic_ai_doc_38 | Tokens: 20 | Preview: Computer vision systems face challenges such as variations in lighting, occlusion, background clutte...\n",
            "    semantic_ai_doc_39 | Tokens: 20 | Preview: ## Ethics and Bias\n",
            "\n",
            "Ethics in AI is a critical aspect that ensures AI technologies are developed and...\n",
            "    semantic_ai_doc_40 | Tokens: 16 | Preview: Ethical considerations include:\n",
            "\n",
            "- **Fairness**: Avoiding discrimination and ensuring equitable trea...\n",
            "    semantic_ai_doc_41 | Tokens: 14 | Preview: - **Transparency**: Making AI systems understandable and explainable so users can trust their decisi...\n",
            "    semantic_ai_doc_42 | Tokens: 9 | Preview: - **Privacy**: Protecting sensitive personal data and preventing misuse.\n",
            "    semantic_ai_doc_43 | Tokens: 13 | Preview: - **Bias Mitigation**: Identifying and reducing biases in training data and AI models.\n",
            "    semantic_ai_doc_44 | Tokens: 9 | Preview: - **Accountability**: Determining responsibility for AI-driven decisions and actions.\n",
            "    semantic_ai_doc_45 | Tokens: 15 | Preview: - **Safety and Security**: Preventing malicious use of AI and ensuring reliability in critical appli...\n",
            "    semantic_ai_doc_46 | Tokens: 20 | Preview: Ethical AI requires collaboration between technologists, policymakers, ethicists, and society to ens...\n",
            "    semantic_ai_doc_47 | Tokens: 30 | Preview: ## Future Directions\n",
            "\n",
            "AI continues to evolve rapidly, with research focused on areas such as:\n",
            "\n",
            "- **G...\n",
            "    semantic_ai_doc_48 | Tokens: 13 | Preview: - **Explainable AI**: Developing models whose decision-making processes can be understood by humans.\n",
            "    semantic_ai_doc_49 | Tokens: 11 | Preview: - **AI in Healthcare**: Improving diagnostics, personalized medicine, and drug discovery.\n",
            "    semantic_ai_doc_50 | Tokens: 16 | Preview: - **AI for Climate and Sustainability**: Leveraging AI for environmental monitoring, resource manage...\n",
            "    semantic_ai_doc_51 | Tokens: 13 | Preview: - **Human-AI Collaboration**: Designing systems that augment human capabilities rather than replace ...\n",
            "    semantic_ai_doc_52 | Tokens: 27 | Preview: The future of AI promises to transform industries, enhance human potential, and address global chall...\n",
            "    semantic_ai_doc_53 | Tokens: 1 | Preview: ...\n",
            "\n",
            "Processing: hybrid\n",
            "  7 chunks, avg tokens: 132.9\n",
            "    heading_ai_doc_0 | Tokens: 108 | Preview: # Artificial Intelligence\n",
            "\n",
            "Artificial Intelligence (AI) is a multidisciplinary field of computer sci...\n",
            "    heading_ai_doc_1 | Tokens: 183 | Preview: ## Machine Learning\n",
            "\n",
            "Machine learning (ML) is a core subset of AI that focuses on creating algorithm...\n",
            "    heading_ai_doc_2 | Tokens: 150 | Preview: ## Natural Language Processing\n",
            "\n",
            "Natural Language Processing (NLP) is a branch of AI that enables com...\n",
            "    heading_ai_doc_3 | Tokens: 121 | Preview: ## Robotics\n",
            "\n",
            "Robotics is an interdisciplinary field that integrates AI with mechanical, electrical, ...\n",
            "    heading_ai_doc_4 | Tokens: 141 | Preview: ## Computer Vision\n",
            "\n",
            "Computer vision is a branch of AI focused on enabling machines to interpret, und...\n",
            "    heading_ai_doc_5 | Tokens: 116 | Preview: ## Ethics and Bias\n",
            "\n",
            "Ethics in AI is a critical aspect that ensures AI technologies are developed and...\n",
            "    heading_ai_doc_6 | Tokens: 111 | Preview: ## Future Directions\n",
            "\n",
            "AI continues to evolve rapidly, with research focused on areas such as:\n",
            "\n",
            "- **G...\n",
            "\n",
            "Starting RAG loop with 'semantic' chunking\n",
            " Query (type 'quit' to exit): what is ml\n",
            "\n",
            " Answer:\n",
            "<think>\n",
            "Okay, let's see. The user is asking \"what is ml\", which stands for Machine Learning. I need to look at the provided context to form the answer.\n",
            "\n",
            "The context starts by defining ML as a core subset of AI focused on algorithms that allow computers to learn from data and improve performance without explicit programming. The categories mentioned are supervised learning and deep learning. It also notes that models are trained using optimization techniques to minimize errors and maximize accuracy.\n",
            "\n",
            "So the answer should start with the basic definition, mention AI, the main goal of learning from data, improving performance, and the categories. Also include the optimization part. Make sure it's concise and covers all key points from the context. Avoid using markdown and keep it in plain text. Let me check again if I missed anything. The user might be looking for a clear, straightforward explanation. Alright, I think that's covered.\n",
            "</think>\n",
            "\n",
            "Machine learning (ML) is a core subset of artificial intelligence (AI) that focuses on developing algorithms enabling computers to learn patterns and make decisions from data without being explicitly programmed. It improves performance over time by analyzing data, identifying patterns, and adapting to new inputs. Key approaches include **supervised learning** (using labeled data for predictions) and **deep learning** (employing multi-layered neural networks for complex data analysis). ML models are trained through optimization techniques to reduce errors and enhance accuracy in tasks like classification, forecasting, and pattern recognition.\n",
            "\n",
            " Retrieved Chunks:\n",
            "1. Score: 0.810, Tokens: 54, Preview: ## Machine Learning\n",
            "\n",
            "Machine learning (ML) is a core subset of AI that focuses on creating algorithms that allow computers to learn from data and impr...\n",
            "2. Score: 1.392, Tokens: 25, Preview: - **Deep Learning**: A subfield of ML based on neural networks with many layers (deep neural networks), capable of learning complex representations fr...\n",
            "3. Score: 1.457, Tokens: 15, Preview: Machine learning models are trained using optimization techniques to minimize errors and maximize predictive accuracy.\n",
            "------------------------------------------------------------\n",
            " Query (type 'quit' to exit): quit\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    docs = {\n",
        "        \"ai_doc\": \"\"\"\n",
        "\n",
        "        # Artificial Intelligence\n",
        "\n",
        "Artificial Intelligence (AI) is a multidisciplinary field of computer science and engineering focused on creating systems and machines capable of performing tasks that typically require human intelligence. These tasks include reasoning, learning, problem-solving, perception, language understanding, decision-making, and interaction with the environment. AI systems leverage large amounts of data, advanced algorithms, and computational power to mimic cognitive functions such as pattern recognition, planning, and adaptation.\n",
        "\n",
        "AI is applied across a wide range of domains including healthcare, finance, education, transportation, entertainment, and scientific research. The ultimate goal of AI is to build machines that can perform complex tasks autonomously and intelligently, while also augmenting human abilities.\n",
        "\n",
        "## Machine Learning\n",
        "\n",
        "Machine learning (ML) is a core subset of AI that focuses on creating algorithms that allow computers to learn from data and improve their performance over time without being explicitly programmed. ML techniques are broadly categorized into:\n",
        "\n",
        "- **Supervised Learning**: Algorithms learn from labeled datasets to make predictions or classify data. Common applications include spam detection, fraud detection, and medical diagnosis.\n",
        "- **Unsupervised Learning**: Algorithms identify patterns or structures in unlabeled data. Examples include clustering customers based on purchasing behavior or anomaly detection in network security.\n",
        "- **Reinforcement Learning**: Agents learn by interacting with an environment and receiving feedback in the form of rewards or penalties. This approach is widely used in robotics, game AI, and autonomous driving.\n",
        "- **Deep Learning**: A subfield of ML based on neural networks with many layers (deep neural networks), capable of learning complex representations from large-scale data. It powers modern AI applications like speech recognition, image classification, and natural language understanding.\n",
        "\n",
        "Machine learning models are trained using optimization techniques to minimize errors and maximize predictive accuracy. Key challenges include overfitting, underfitting, interpretability, and scalability.\n",
        "\n",
        "## Natural Language Processing\n",
        "\n",
        "Natural Language Processing (NLP) is a branch of AI that enables computers to understand, interpret, and generate human language. NLP combines computational linguistics, machine learning, and statistical methods to process text, speech, and other forms of language data. Common applications include:\n",
        "\n",
        "- **Chatbots and Virtual Assistants**: AI-powered conversational agents like virtual assistants, customer service bots, and interactive agents.\n",
        "- **Sentiment Analysis**: Determining opinions or emotions in text for marketing, social media monitoring, and customer feedback.\n",
        "- **Language Translation**: Translating text or speech between languages using AI models like neural machine translation.\n",
        "- **Text Summarization**: Automatically creating concise summaries of large documents or articles.\n",
        "- **Speech Recognition and Generation**: Converting spoken language to text and vice versa for accessibility, transcription, and interactive applications.\n",
        "\n",
        "NLP also involves challenges such as understanding context, idioms, sarcasm, and ambiguity in human language, as well as addressing biases present in language data.\n",
        "\n",
        "## Robotics\n",
        "\n",
        "Robotics is an interdisciplinary field that integrates AI with mechanical, electrical, and software engineering to create autonomous and semi-autonomous machines. Robotics systems can sense their environment, process information, make decisions, and perform physical actions. Key areas include:\n",
        "\n",
        "- **Industrial Robotics**: Robots used in manufacturing, assembly lines, packaging, and precision tasks.\n",
        "- **Service Robotics**: Robots designed for healthcare, domestic help, hospitality, and customer service.\n",
        "- **Autonomous Vehicles**: Self-driving cars, drones, and delivery robots that navigate without human intervention.\n",
        "- **Humanoid Robots**: Robots designed to resemble humans and interact in human-like ways.\n",
        "- **Medical Robotics**: Surgical robots, rehabilitation systems, and assistive devices for patients.\n",
        "\n",
        "Challenges in robotics include real-time perception, mobility in unstructured environments, manipulation of objects, human-robot interaction, and safety.\n",
        "\n",
        "## Computer Vision\n",
        "\n",
        "Computer vision is a branch of AI focused on enabling machines to interpret, understand, and respond to visual data from the world. It combines image processing, pattern recognition, and machine learning techniques to extract meaningful information from images and videos. Common tasks and applications include:\n",
        "\n",
        "- **Object Detection**: Identifying and localizing objects within images or videos.\n",
        "- **Image Classification**: Categorizing images into predefined classes for recognition purposes.\n",
        "- **Facial Recognition**: Identifying or verifying individuals based on facial features.\n",
        "- **Medical Imaging Analysis**: Detecting diseases and anomalies in X-rays, MRIs, and CT scans.\n",
        "- **Autonomous Navigation**: Allowing vehicles and robots to perceive their surroundings for safe navigation.\n",
        "- **Augmented Reality and Computer Graphics**: Enhancing real-world environments with computer-generated imagery.\n",
        "\n",
        "Computer vision systems face challenges such as variations in lighting, occlusion, background clutter, and the need for large annotated datasets.\n",
        "\n",
        "## Ethics and Bias\n",
        "\n",
        "Ethics in AI is a critical aspect that ensures AI technologies are developed and deployed responsibly. Ethical considerations include:\n",
        "\n",
        "- **Fairness**: Avoiding discrimination and ensuring equitable treatment across different groups of people.\n",
        "- **Transparency**: Making AI systems understandable and explainable so users can trust their decisions.\n",
        "- **Privacy**: Protecting sensitive personal data and preventing misuse.\n",
        "- **Bias Mitigation**: Identifying and reducing biases in training data and AI models.\n",
        "- **Accountability**: Determining responsibility for AI-driven decisions and actions.\n",
        "- **Safety and Security**: Preventing malicious use of AI and ensuring reliability in critical applications.\n",
        "\n",
        "Ethical AI requires collaboration between technologists, policymakers, ethicists, and society to ensure that AI benefits humanity while minimizing potential harms.\n",
        "\n",
        "## Future Directions\n",
        "\n",
        "AI continues to evolve rapidly, with research focused on areas such as:\n",
        "\n",
        "- **General AI**: Creating machines with human-level cognitive abilities across a broad range of tasks.\n",
        "- **Explainable AI**: Developing models whose decision-making processes can be understood by humans.\n",
        "- **AI in Healthcare**: Improving diagnostics, personalized medicine, and drug discovery.\n",
        "- **AI for Climate and Sustainability**: Leveraging AI for environmental monitoring, resource management, and climate modeling.\n",
        "- **Human-AI Collaboration**: Designing systems that augment human capabilities rather than replace them.\n",
        "\n",
        "The future of AI promises to transform industries, enhance human potential, and address global challenges, but it also requires careful consideration of ethical, social, and economic impacts.\n",
        "...\n",
        "    \"\"\"}\n",
        "    llm = LLMClient()\n",
        "    llm.set_api_key_interactive()\n",
        "\n",
        "    rag = RAGPipeline()\n",
        "    rag.llm = llm\n",
        "\n",
        "    rag.process_documents(docs)\n",
        "\n",
        "    rag.rag_loop(method='semantic', k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whV3Ymhj-8A0"
      },
      "source": [
        "#Inference :\n",
        "\n",
        "Fixed Size :\n",
        "Very few, large chunks. May contain multiple topics per chunk. Can lead to mixed context but fewer chunks to search.\n",
        "\n",
        "Sentence Based :\n",
        "Small, precise chunks (mostly single sentences). Less likely to break thoughts within a sentence. May require combining multiple chunks for complex queries.\n",
        "\n",
        "Paragraph Based :\n",
        "Each paragraph forms a chunk. Preserves some context while still small. Good for moderate granularity.\n",
        "\n",
        "Sliding Window :\n",
        "Large overlapping chunks. Preserves context across boundaries. Balanced between context and search efficiency.\n",
        "\n",
        "Heading Based :\n",
        "Each heading/section forms a chunk. Preserves semantic grouping. Moderate number of chunks; usually keeps thoughts intact.\n",
        "\n",
        "Semantic :\n",
        "Very fine-grained; splits content into semantic clusters. Most chunks retrieved are highly relevant. Can be fragmented; single sentences or small clusters.\n",
        "\n",
        "Hybrid :\n",
        "Combines heading-based and sliding window. Context-preserving with reasonable chunk count.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Which strategy retrieves more relevant chunks :\n",
        "Semantic retrieved 3 highly relevant chunks: definition, deep learning, optimization. Heading/Hybrid retrieve 1–2 chunks covering the main section. Fixed Size/Sliding Window may retrieve large chunks containing relevant info plus some noise. Sentence/Paragraph based retrieve many small chunks, may need combining to answer fully.\n",
        "\n",
        "#Which leads to fewer “broken thoughts” :\n",
        "Heading/Hybrid/Sliding Window maintain coherent ideas; fewer interruptions. Fixed Size chunks are large enough to preserve context, but may mix multiple topics. Sentence/Semantic can fragment information into very small pieces, breaking ideas.\n",
        "\n",
        "\n",
        "#conclusion\n",
        "Use Semantic for high precision and maximum relevance.\n",
        "\n",
        "Use Heading or Hybrid for coherent chunks and natural flow of ideas.\n",
        "\n",
        "Use Sliding Window for a balanced approach, retaining context without too much fragmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkRYZ2511E3Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
